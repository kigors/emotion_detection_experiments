{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coated-helmet",
   "metadata": {},
   "source": [
    "### Данный блокнот создан для классификации тестовых изображений и выгрузки результатов в требуемом формате для Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "approved-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "print('tensorflow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-covering",
   "metadata": {},
   "source": [
    "Функция для вырезания лица на основании предсказаний opencv_face_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "infrared-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate face detector\n",
    "folder = 'face_detection_data/'\n",
    "fd_net =  cv2.dnn.readNetFromCaffe(\n",
    "                folder + 'opencv_face_detector.prototxt', \n",
    "                folder + 'opencv_face_detector.caffemodel'\n",
    "            )\n",
    "\n",
    "def get_faces(image):\n",
    "    \"\"\"\n",
    "    Run inference of face detector and return list of boxes with faces.\n",
    "    Image is ndarray in standard RGB format with 0-255 values for color.\n",
    "    \"\"\"\n",
    "    (h, w) = image.shape[:2]\n",
    " \n",
    "    # preprocess image, add batch dim, rearrange dimentions (batch,channels,h,w)\n",
    "    # size, mean and scale are determined by face detector \n",
    "    # (https://github.com/opencv/opencv/blob/master/samples/dnn/models.yml)\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        image=image, \n",
    "        scalefactor=1.0,\n",
    "        size=(300, 300), \n",
    "        mean=(104.0, 177.0, 123.0),\n",
    "        swapRB=True,\n",
    "    )\n",
    "    \n",
    "    # run inference of face detector\n",
    "    fd_net.setInput(blob)\n",
    "    detections = fd_net.forward()\n",
    "    \n",
    "    # retrieve face boxes\n",
    "    faces = []\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            faces.append(box.astype(\"int\"))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-cartridge",
   "metadata": {},
   "source": [
    "Функция для загрузки изображения и преобразования его по требуемому размеру с опцией вырезания лица, если модель обучалась на вырезанных лицах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extreme-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image(filename, img_size=None, detector_flag=False):\n",
    "    \"\"\"\n",
    "    Load test image and prepare it for classifier:\n",
    "        - resize to img_size\n",
    "        - trim image to face box as per face detector prediction\n",
    "          It always takes the largest face.\n",
    "          No cut if there's no face detected.\n",
    "    Returns image as ndarray (h, w, 3), where channel values are in range [0, 1]\n",
    "    \"\"\"\n",
    "    with Image.open(filename) as img:\n",
    "        image = img.convert(\"RGB\")\n",
    "        \n",
    "        if detector_flag:\n",
    "            image_np = np.array(image)\n",
    "            boxes = get_faces(image_np)\n",
    "            # cut a face if we detect a face otherwise skip\n",
    "            if len(boxes) != 0:\n",
    "                # find box index with the largest area\n",
    "                max_area = 0\n",
    "                max_box_idx = 0\n",
    "                for i, box in enumerate(boxes):\n",
    "                    startW, startH, endW, endH = box\n",
    "                    area = np.abs((endW - startW) * (endH - startH))\n",
    "                    if area > max_area:\n",
    "                        max_area = area\n",
    "                        max_box_idx = i\n",
    "\n",
    "                startW, startH, endW, endH = boxes[max_box_idx]\n",
    "                face = image_np[startH:endH, startW:endW]\n",
    "                image = Image.fromarray(face)\n",
    "   \n",
    "        image = image.resize((img_size, img_size))\n",
    "        return np.array(image) / 255.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-packaging",
   "metadata": {},
   "source": [
    "Функция создания csv файла. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "governing-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submition_file(model, img_size, detector_flag):\n",
    "    \"\"\"\n",
    "    model - trained model\n",
    "    img_size - size to resize images to\n",
    "    detector_flag - True if model was trained on case2 dataset, otherwise - False\n",
    "    \"\"\"\n",
    "    # labels are always the same so to cpu time, I hardcoded this dict\n",
    "    labels_dict = {\n",
    "        0: 'anger',\n",
    "        1: 'contempt',\n",
    "        2: 'disgust',\n",
    "        3: 'fear',\n",
    "        4: 'happy',\n",
    "        5: 'neutral',\n",
    "        6: 'sad',\n",
    "        7: 'surprise',\n",
    "        8: 'uncertain'\n",
    "    }\n",
    "    \n",
    "    # get files in test_kaggle folder (assuming there's no extra files)\n",
    "    files = [str(name) for  name in test_dir.glob('*')]\n",
    "    \n",
    "    # create DataFrame with classification results\n",
    "    results = pd.DataFrame(files, columns=['path'])\n",
    "    results['image_path'] = [x.split('/')[-1] for x in files] # filename only\n",
    "    results['emotion'] = 'nan' # 'nan' is used for asserting later on\n",
    "    BATCH_SIZE = 100\n",
    "    i = 0\n",
    "    print('Evaluating test data...\\nSteps:')\n",
    "    while i + BATCH_SIZE <= results.shape[0]:\n",
    "        # construct batch manually reading filenames by amount of BATCH_SIZE \n",
    "        names = results.iloc[i:i+BATCH_SIZE]['path'].tolist()\n",
    "        images = []\n",
    "        for name in names:\n",
    "            images.append(load_and_process_image(name, img_size, detector_flag))\n",
    "        batch = np.array(images)\n",
    "        \n",
    "        # make prediction on a batch\n",
    "        predictions = model(batch)\n",
    "        y_pred = np.argmax(predictions, axis=-1)\n",
    "        \n",
    "        # write predicted class to DataFrame\n",
    "        results.iloc[i:i+BATCH_SIZE, 2] = [labels_dict[x] for x in y_pred]\n",
    "        i += BATCH_SIZE\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print('  ', i)\n",
    "    print('... Done.')\n",
    "    \n",
    "    # sort dataframe by filename\n",
    "    results.sort_values('image_path', ascending=True, inplace=True)\n",
    "    # export to csv as per requirements\n",
    "    columns = ['image_path', 'emotion']\n",
    "    results[columns].to_csv('results.csv', columns=columns, index=False)\n",
    "    \n",
    "    # simple assert that we didn't miss a file. \n",
    "    # It could happen if number of files is not multiple of BATCH_SIZE... I bit lazy to make it universal.\n",
    "    assert not ('nan' in results.emotion.unique()), \"some images left unprocessed/unevaluated...\"\n",
    "    print('results.csv was created.')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-michael",
   "metadata": {},
   "source": [
    "Протестируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bound-friendly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/bit-m/chpt1 checkpoint loaded.\n",
      "Evaluating test data...\n",
      "Steps:\n",
      "   1000\n",
      "   2000\n",
      "   3000\n",
      "   4000\n",
      "   5000\n",
      "... Done.\n",
      "results.csv was created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4634</th>\n",
       "      <td>dataset/test_kaggle/0.jpg</td>\n",
       "      <td>0.jpg</td>\n",
       "      <td>sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3638</th>\n",
       "      <td>dataset/test_kaggle/1.jpg</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>dataset/test_kaggle/10.jpg</td>\n",
       "      <td>10.jpg</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2137</th>\n",
       "      <td>dataset/test_kaggle/100.jpg</td>\n",
       "      <td>100.jpg</td>\n",
       "      <td>uncertain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>dataset/test_kaggle/1000.jpg</td>\n",
       "      <td>1000.jpg</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path image_path    emotion\n",
       "4634     dataset/test_kaggle/0.jpg      0.jpg        sad\n",
       "3638     dataset/test_kaggle/1.jpg      1.jpg    neutral\n",
       "2582    dataset/test_kaggle/10.jpg     10.jpg    neutral\n",
       "2137   dataset/test_kaggle/100.jpg    100.jpg  uncertain\n",
       "2902  dataset/test_kaggle/1000.jpg   1000.jpg      happy"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set folder with test images\n",
    "test_dir = Path('dataset') / 'test_kaggle'\n",
    "\n",
    "# choose folder with saved model being tested\n",
    "chpt_name = 'models/bit-m/chpt1'\n",
    "# load model\n",
    "model = tf.keras.models.load_model(chpt_name)\n",
    "print(f\"{chpt_name} checkpoint loaded.\")\n",
    "\n",
    "# test and export results\n",
    "result_df = generate_submition_file(model, img_size=200, detector_flag=False)\n",
    "result_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd031b91c49bde7e3c6fc9213e05887f777d08744027ab71904dfeec791132a5b07"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
